!> \file
!> $Id$
!> \author Chris Bradley
!> \brief This module contains all computational environment variables.
!>
!> \section LICENSE
!>
!> Version: MPL 1.1/GPL 2.0/LGPL 2.1
!>
!> The contents of this file are subject to the Mozilla Public License
!> Version 1.1 (the "License"); you may not use this file except in
!> compliance with the License. You may obtain a copy of the License at
!> http://www.mozilla.org/MPL/
!>
!> Software distributed under the License is distributed on an "AS IS"
!> basis, WITHOUT WARRANTY OF ANY KIND, either express or implied. See the
!> License for the specific language governing rights and limitations
!> under the License.
!>
!> The Original Code is OpenCMISS
!>
!> The Initial Developer of the Original Code is University of Auckland,
!> Auckland, New Zealand and University of Oxford, Oxford, United
!> Kingdom. Portions created by the University of Auckland and University
!> of Oxford are Copyright (C) 2007 by the University of Auckland and
!> the University of Oxford. All Rights Reserved.
!>
!> Contributor(s):
!>
!> Alternatively, the contents of this file may be used under the terms of
!> either the GNU General Public License Version 2 or later (the "GPL"), or
!> the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
!> in which case the provisions of the GPL or the LGPL are applicable instead
!> of those above. If you wish to allow use of your version of this file only
!> under the terms of either the GPL or the LGPL, and not to allow others to
!> use your version of this file under the terms of the MPL, indicate your
!> decision by deleting the provisions above and replace them with the notice
!> and other provisions required by the GPL or the LGPL. If you do not delete
!> the provisions above, a recipient may use your version of this file under
!> the terms of any one of the MPL, the GPL or the LGPL.
!>

!> This module contains all computational environment variables.

MODULE COMP_ENVIRONMENT

  USE BASE_ROUTINES
  USE CMISS_MPI
  USE CMISS_PETSC
  USE CONSTANTS
  USE KINDS
  USE MPI
  USE INPUT_OUTPUT
  USE ISO_VARYING_STRING
  USE TYPES
 
  IMPLICIT NONE
 

  !Module parameters

  !Module types  

  !Module variables

  TYPE(COMPUTATIONAL_ENVIRONMENT_TYPE),TARGET :: COMPUTATIONAL_ENVIRONMENT !<The computational environment the program is running in.
  TYPE(MPI_COMPUTATIONAL_NODE_TYPE) :: MPI_COMPUTATIONAL_NODE_TYPE_DATA !<The MPI data on the computational nodes.
  TYPE(COMPUTATIONAL_WORK_GROUP_TYPE), POINTER :: COMPUTATIONAL_WORLD_WORK_GROUP !<The computational world work group the program is running in.
  TYPE(COMPUTATIONAL_WORK_GROUP_TYPE), POINTER :: COMPUTATIONAL_LOCAL_WORK_GROUP !<The computational local work group the program is running in.

  !Interfaces

  !>Returns the number of computational nodes.
  INTERFACE COMPUTATIONAL_NODES_NUMBER_GET
    MODULE PROCEDURE COMPUTATIONAL_NODES_NUMBER_GET_WORLD_WORKGROUP
    MODULE PROCEDURE COMPUTATIONAL_NODES_NUMBER_GET_WORKGROUP
  END INTERFACE 

  !>Returns the rank of the computational node.
  INTERFACE COMPUTATIONAL_NODE_NUMBER_GET
    MODULE PROCEDURE COMPUTATIONAL_NODE_NUMBER_GET_WORLD_WORKGROUP
    MODULE PROCEDURE COMPUTATIONAL_NODE_NUMBER_GET_WORKGROUP
  END INTERFACE 

  !>Set a barrier point
  INTERFACE  COMPUTATIONAL_WORK_GROUP_WAIT
    MODULE PROCEDURE COMPUTATIONAL_WORK_GROUP_WAIT_WORLD_WORKGROUP
!    MODULE PROCEDURE COMPUTATIONAL_WORK_GROUP_WAIT_WORKGROUP
  END INTERFACE 


  ! Access specifiers for subroutines and interfaces(if any)
  PRIVATE COMPUTATIONAL_LOCAL_WORK_GROUP,COMPUTATIONAL_WORLD_WORK_GROUP !for further modification, keep it private
  PUBLIC COMPUTATIONAL_ENVIRONMENT
  PUBLIC COMPUTATIONAL_ENVIRONMENT_INITIALISE,COMPUTATIONAL_ENVIRONMENT_FINALISE,COMPUTATIONAL_NODES_NUMBER_GET, &
    & COMPUTATIONAL_NODE_NUMBER_GET, COMPUTATIONAL_WORK_GROUP_GET_LOCAL_WORK_GROUP
  PUBLIC COMPUTATIONAL_WORK_GROUP_SUBGROUP_ADD, COMPUTATIONAL_WORK_GROUP_CREATE_START, COMPUTATIONAL_WORK_GROUP_CREATE_FINISH
  PUBLIC COMPUTATIONAL_WORK_GROUP_WAIT

CONTAINS


  !================================================================================================================================
  !
  !>
  !>CREATE THE COMMINCATOR OF THIS WORK GROUP (DEFAULT: MPI_COMM FOR WORLD WORLD GROUP)
  !>
  FUNCTION COMPUTATIONAL_WORK_GROUP_COMMUNICATOR_GET(WORK_GROUP,ERR,ERROR)
    !Argument Variables
    TYPE(COMPUTATIONAL_WORK_GROUP_TYPE),POINTER, INTENT(IN) :: WORK_GROUP
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Function variable
     INTEGER(INTG) :: COMPUTATIONAL_WORK_GROUP_COMMUNICATOR_GET !<The MPI communicator ID
        
    IF(ASSOCIATED(WORK_GROUP)) THEN 
        IF(WORK_GROUP%COMP_ENV_FINISHED) THEN
          COMPUTATIONAL_WORK_GROUP_COMMUNICATOR_GET = WORK_GROUP%COMP_ENVIRONMENT%MPI_COMM  
        ELSE
          CALL FLAG_ERROR('computational environment of this work group has not finished', ERR, ERROR, *999)
        ENDIF
    ELSE
     CALL FLAG_ERROR('WORK_GROUP is not associated', ERR, ERROR, *999)
    ENDIF
    
    CALL EXITS("COMPUTATIONAL_WORK_GROUP_COMMUNICATOR_GET")
    RETURN
999 CALL ERRORS("COMPUTATIONAL_WORK_GROUP_COMMUNICATOR_GET",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_WORK_GROUP_COMMUNICATOR_GET")
    RETURN 
  END FUNCTION COMPUTATIONAL_WORK_GROUP_COMMUNICATOR_GET

  !================================================================================================================================
  !
  !>
  !>Get the local working group(s) that this rank belongs to (current restriction: one rank can only belong to one local work group)
  !>
  SUBROUTINE COMPUTATIONAL_WORK_GROUP_GET_LOCAL_WORK_GROUP(LOCAL_WORK_GROUP,ERR,ERROR,*)
    !Argument Variables
    TYPE(COMPUTATIONAL_WORK_GROUP_TYPE),POINTER, INTENT(INOUT) :: LOCAL_WORK_GROUP
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string

    CALL ENTERS("COMPUTATIONAL_WORK_GROUP_GET_LOCAL_WORK_GROUP",ERR,ERROR,*999)

    IF(ASSOCIATED(COMPUTATIONAL_LOCAL_WORK_GROUP)) THEN
      LOCAL_WORK_GROUP => COMPUTATIONAL_LOCAL_WORK_GROUP
    ELSE
      CALL FLAG_ERROR('COMPUTATIONAL_LOCAL_WORK_GROUP is not accociated, &
       &  THIS PROCESS/RANK DOES NOT BEGLONG TO ANY LOCAL WORK GROUP', ERR, ERROR, *999)
    ENDIF
    
    CALL EXITS("COMPUTATIONAL_WORK_GROUP_GET_LOCAL_WORK_GROUP")
    RETURN
999 CALL ERRORS("COMPUTATIONAL_WORK_GROUP_GET_LOCAL_WORK_GROUP",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_WORK_GROUP_GET_LOCAL_WORK_GROUP")
    RETURN 1
  END SUBROUTINE COMPUTATIONAL_WORK_GROUP_GET_LOCAL_WORK_GROUP

  !================================================================================================================================
  !
  !>
  !>Get the local working group(s) that this rank belongs to (current restriction: one rank can only belong to one local work group)
  !>
  SUBROUTINE COMPUTATIONAL_WORK_GROUP_GET_DEFAULT_WORK_GROUP(DEFAULT_WORK_GROUP,ERR,ERROR,*)
    !Argument Variables
    TYPE(COMPUTATIONAL_WORK_GROUP_TYPE),POINTER, INTENT(INOUT) :: DEFAULT_WORK_GROUP
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string

    CALL ENTERS("COMPUTATIONAL_WORK_GROUP_GET_DEFAULT_WORK_GROUP",ERR,ERROR,*999)

    IF(ASSOCIATED(COMPUTATIONAL_WORLD_WORK_GROUP)) THEN
      DEFAULT_WORK_GROUP => COMPUTATIONAL_WORLD_WORK_GROUP
    ELSE
      CALL FLAG_ERROR('COMPUTATIONAL_LOCAL_WORK_GROUP is not accociated, &
       &  THIS PROCESS/RANK DOES NOT BEGLONG TO ANY LOCAL WORK GROUP', ERR, ERROR, *999)
    ENDIF
    
    CALL EXITS("COMPUTATIONAL_WORK_GROUP_GET_DEFAULT_WORK_GROUP")
    RETURN
999 CALL ERRORS("COMPUTATIONAL_WORK_GROUP_GET_DEFAULT_WORK_GROUP",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_WORK_GROUP_GET_DEFAULT_WORK_GROUP")
    RETURN 1
  END SUBROUTINE COMPUTATIONAL_WORK_GROUP_GET_DEFAULT_WORK_GROUP

  !
  !================================================================================================================================
  !
  !>
  !>CREATE THE HIGHEST LEVEL WORK GROUP (DEFAULT: GROUP_WORLD)
  !>
  SUBROUTINE COMPUTATIONAL_WORK_GROUP_CREATE_START(USER_NUMBER, WORLD_WORK_GROUP,NUMBER_COMPUTATIONAL_NODES,ERR,ERROR,*)
    !Argument Variables
    INTEGER(INTG), INTENT(IN) :: USER_NUMBER !<The user number of the WORLD WorkGroup to start the creation of.
    TYPE(COMPUTATIONAL_WORK_GROUP_TYPE),POINTER, INTENT(INOUT) :: WORLD_WORK_GROUP
    INTEGER(INTG),INTENT(IN) :: NUMBER_COMPUTATIONAL_NODES
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !LOCAL VARIABLES
    TYPE(COMPUTATIONAL_WORK_GROUP_PTR_TYPE) NEW_WORK_GROUP

    CALL ENTERS("COMPUTATIONAL_WORK_GROUP_CREATE_START",ERR,ERROR,*999)

    IF(ASSOCIATED(WORLD_WORK_GROUP)) THEN 
      CALL FLAG_ERROR('WORLD_WORK_GROUP IS ALREADY associated', ERR, ERROR, *999)
    ELSE
      ALLOCATE(NEW_WORK_GROUP%PTR)
      NEW_WORK_GROUP%PTR%NUMBER_COMPUTATIONAL_NODES = NUMBER_COMPUTATIONAL_NODES
      NEW_WORK_GROUP%PTR%NUMBER_SUB_WORK_GROUPS = 0   
      NULLIFY(NEW_WORK_GROUP%PTR%PARENT) !IT IS THE TOPEST LEVEL WORK GROUP ALREADY 
      NULLIFY(NEW_WORK_GROUP%PTR%COMP_ENVIRONMENT) !GENERATE THIS LATER IN COMPUTATIONAL_WORK_GROUP_CREATE_FINISH
      WORLD_WORK_GROUP  => NEW_WORK_GROUP%PTR
    ENDIF

    COMPUTATIONAL_WORLD_WORK_GROUP  => WORLD_WORK_GROUP
    CALL EXITS("COMPUTATIONAL_WORK_GROUP_CREATE_START")
    RETURN
999 CALL ERRORS("COMPUTATIONAL_WORK_GROUP_CREATE_START",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_WORK_GROUP_CREATE_START")
    RETURN 1
  END SUBROUTINE COMPUTATIONAL_WORK_GROUP_CREATE_START

  !
  !================================================================================================================================
  !
  !>
  !>ADD WORK SUB-GROUP TO THE PARENT GROUP BASED ON THE COMPUTATIONAL REQUIREMENTS (CALLED BY THE USER)
  !>
  SUBROUTINE COMPUTATIONAL_WORK_GROUP_SUBGROUP_ADD(USER_NUMBER_ADDED_GROUP, PARENT_WORK_GROUP, NUMBER_COMPUTATIONAL_NODES, &
   & ADDED_WORK_GROUP,ERR,ERROR,*)
    !Argument Variables
    INTEGER(INTG), INTENT(IN) :: USER_NUMBER_ADDED_GROUP !<The user number of the WorkGroup to ADD
    TYPE(COMPUTATIONAL_WORK_GROUP_TYPE),POINTER, INTENT(INOUT) :: PARENT_WORK_GROUP
    TYPE(COMPUTATIONAL_WORK_GROUP_TYPE),POINTER, INTENT(INOUT) :: ADDED_WORK_GROUP
    INTEGER(INTG),INTENT(IN) :: NUMBER_COMPUTATIONAL_NODES
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !local variables
    INTEGER(INTG) :: NGROUP
    TYPE(COMPUTATIONAL_WORK_GROUP_PTR_TYPE) NEW_WORK_GROUP
    TYPE(COMPUTATIONAL_WORK_GROUP_TYPE),POINTER ::  TMP_PARENT_WORK_GROUP
    TYPE(COMPUTATIONAL_WORK_GROUP_PTR_TYPE), ALLOCATABLE :: SUB_WORK_GROUPS(:)
    INTEGER(INTG):: I

    CALL ENTERS("COMPUTATIONAL_WORK_GROUP_SUBGROUP_ADD",ERR,ERROR,*999)

    ALLOCATE(NEW_WORK_GROUP%PTR)
    NEW_WORK_GROUP%PTR%NUMBER_COMPUTATIONAL_NODES = NUMBER_COMPUTATIONAL_NODES
    NEW_WORK_GROUP%PTR%NUMBER_SUB_WORK_GROUPS = 0

    IF(ASSOCIATED(PARENT_WORK_GROUP)) THEN 
      ALLOCATE(SUB_WORK_GROUPS(PARENT_WORK_GROUP%NUMBER_SUB_WORK_GROUPS+1))
      DO I=1,PARENT_WORK_GROUP%NUMBER_SUB_WORK_GROUPS
        SUB_WORK_GROUPS(I)%PTR=>PARENT_WORK_GROUP%SUB_WORK_GROUPS(I)%PTR
      ENDDO
      !SUB_WORK_GROUPS(1:PARENT_WORK_GROUP%NUMBER_SUB_WORK_GROUPS)=>PARENT_WORK_GROUP%SUB_WORK_GROUPS(:)
      
      IF(ALLOCATED(PARENT_WORK_GROUP%SUB_WORK_GROUPS)) THEN 
        DEALLOCATE(PARENT_WORK_GROUP%SUB_WORK_GROUPS)
      ENDIF
      SUB_WORK_GROUPS(1+PARENT_WORK_GROUP%NUMBER_SUB_WORK_GROUPS)%PTR=>NEW_WORK_GROUP%PTR
      ALLOCATE(PARENT_WORK_GROUP%SUB_WORK_GROUPS(SIZE(SUB_WORK_GROUPS,1)))
      DO I=1,SIZE(SUB_WORK_GROUPS,1)
        PARENT_WORK_GROUP%SUB_WORK_GROUPS(I)%PTR => SUB_WORK_GROUPS(I)%PTR
      ENDDO
      !PARENT_WORK_GROUP%SUB_WORK_GROUPS(:) => SUB_WORK_GROUPS(:)
      
      DEALLOCATE(SUB_WORK_GROUPS)
      PARENT_WORK_GROUP%NUMBER_SUB_WORK_GROUPS = 1+PARENT_WORK_GROUP%NUMBER_SUB_WORK_GROUPS
      NEW_WORK_GROUP%PTR%PARENT => PARENT_WORK_GROUP
      TMP_PARENT_WORK_GROUP => PARENT_WORK_GROUP 
      DO WHILE(ASSOCIATED(TMP_PARENT_WORK_GROUP)) !UPDATE THE COMPUTATIONAL NUMBER OF ITS ANCESTORS
        TMP_PARENT_WORK_GROUP%NUMBER_COMPUTATIONAL_NODES = TMP_PARENT_WORK_GROUP%NUMBER_COMPUTATIONAL_NODES &
          & + NEW_WORK_GROUP%PTR%NUMBER_COMPUTATIONAL_NODES
        TMP_PARENT_WORK_GROUP => TMP_PARENT_WORK_GROUP%PARENT
      ENDDO
    ELSE !TOP-LEVEL GROUP 
      CALL FLAG_ERROR('PARENT_WORK_GROUP IS NOT associated, CALL COMPUTATIONAL_WORK_GROUP_CREATE_START FIRST',&
      & ERR, ERROR, *999)
    ENDIF
    ADDED_WORK_GROUP => NEW_WORK_GROUP%PTR

    CALL EXITS("COMPUTATIONAL_WORK_GROUP__SUBGROU")
    RETURN
999 CALL ERRORS("COMPUTATIONAL_WORK_GROUP_SUBGROUP_ADD",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_WORK_GROUP_SUBGROUP_ADD")
    RETURN 1
  END SUBROUTINE COMPUTATIONAL_WORK_GROUP_SUBGROUP_ADD

  !
  !================================================================================================================================
  !
  !>
  !>GENERATE COMPUTATIONAL ENVIRONMENT for current level WORK GROUP TREE and all its subgroups recursively 
  !>
  RECURSIVE SUBROUTINE COMPUTATIONAL_WORK_GROUP_GENERATE_COMP_ENVIRONMENT(WORK_GROUP,AVAILABLE_RANK_LIST,ERR,ERROR,*)
    !Argument Variables
    TYPE(COMPUTATIONAL_WORK_GROUP_TYPE),POINTER, INTENT(INOUT) :: WORK_GROUP
    INTEGER(INTG), ALLOCATABLE, INTENT(INOUT) :: AVAILABLE_RANK_LIST(:)
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string

    !LOCAL VARIABLES
    INTEGER(INTG) :: I,DUMMY_ERR,MPI_IERROR,RANK, ORIGINAL_GROUP, NEW_GROUP,BCAST_VALUE
    TYPE(VARYING_STRING) :: DUMMY_ERROR
    INTEGER(INTG), ALLOCATABLE :: NEW_AVAILABLE_RANK_LIST(:)
    INTEGER(INTG) :: MY_COMPUTATIONAL_NODE_NUMBER_IN_WORLD !<the global rank of the current process.
    
    CALL ENTERS("COMPUTATIONAL_WORK_GROUP_GENERATE_COMP_ENVIRONMENT",ERR,ERROR,*999)

    ALLOCATE(WORK_GROUP%COMP_ENVIRONMENT)

    !initialize the groups and communicators to be null (very important and alwayds a good habit to do so)
    ORIGINAL_GROUP=MPI_GROUP_NULL
    NEW_GROUP=MPI_GROUP_NULL
    WORK_GROUP%COMP_ENVIRONMENT%MPI_COMM=MPI_COMM_NULL

    MY_COMPUTATIONAL_NODE_NUMBER_IN_WORLD = COMPUTATIONAL_NODE_NUMBER_GET(ERR,ERROR)

    !Set size of compuational nodes in this communicator
    WORK_GROUP%COMP_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES = WORK_GROUP%NUMBER_COMPUTATIONAL_NODES
    
    !Create a communicator !careful: only the ranks that fall into this group CAN create this group, otherwise you will get a MPI_COMM_NULL
    CALL MPI_COMM_GROUP(MPI_COMM_WORLD,ORIGINAL_GROUP,MPI_IERROR);
    CALL MPI_BARRIER(MPI_COMM_WORLD, MPI_IERROR)
    
    IF(MPI_GROUP_NULL==ORIGINAL_GROUP) THEN
      IF(DIAGNOSTICS5) THEN
        WRITE(OP_STRING,'("(I):warning: CAN NOT CREATE ORIGINAL_GROUP")')  MY_COMPUTATIONAL_NODE_NUMBER_IN_WORLD
        CALL WRITE_STR(DIAGNOSTIC_OUTPUT_TYPE,ERR,ERROR,*999)
      ENDIF          
    ENDIF
    CALL MPI_ERROR_CHECK("MPI_COMM_RANK",MPI_IERROR,ERR,ERROR,*999)

    IF(SIZE(AVAILABLE_RANK_LIST,1)-WORK_GROUP%COMP_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES < 0) THEN
      CALL FLAG_ERROR("NOT ENOUGH RANKS", ERR, ERROR, *999)
      GOTO 999
    ENDIF

    I=WORK_GROUP%COMP_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES
    CALL MPI_GROUP_INCL(ORIGINAL_GROUP,I,AVAILABLE_RANK_LIST(1:I),NEW_GROUP,MPI_IERROR)    
    CALL MPI_BARRIER(MPI_COMM_WORLD, MPI_IERROR)
    IF(MPI_GROUP_NULL==NEW_GROUP) THEN
      IF(DIAGNOSTICS5) THEN
        WRITE(OP_STRING,'("(I): warning: CAN NOT CREATE NEW_GROUP")')  MY_COMPUTATIONAL_NODE_NUMBER_IN_WORLD
        CALL WRITE_STR(DIAGNOSTIC_OUTPUT_TYPE,ERR,ERROR,*999)
      ENDIF     
    ENDIF
    CALL MPI_ERROR_CHECK("MPI_COMM_RANK",MPI_IERROR,ERR,ERROR,*999)    
    CALL MPI_COMM_CREATE(MPI_COMM_WORLD,NEW_GROUP,WORK_GROUP%COMP_ENVIRONMENT%MPI_COMM,MPI_IERROR)
    CALL MPI_BARRIER(MPI_COMM_WORLD, MPI_IERROR)

    IF(MPI_COMM_NULL==WORK_GROUP%COMP_ENVIRONMENT%MPI_COMM) THEN
      IF(DIAGNOSTICS5) THEN
        WRITE(OP_STRING,'("(I): warning: CAN NOT CREATE COMMUNICATOR because this rank does not belong to this group")')  &
         & MY_COMPUTATIONAL_NODE_NUMBER_IN_WORLD
        CALL WRITE_STR(DIAGNOSTIC_OUTPUT_TYPE,ERR,ERROR,*999)
      ENDIF     
    ELSE
      !test/debug
      !BCAST_VALUE=MY_COMPUTATIONAL_NODE_NUMBER_IN_WORLD
      !CALL MPI_BCAST(BCAST_VALUE,1,MPI_INTEGER,0,WORK_GROUP%COMP_ENVIRONMENT%MPI_COMM,MPI_IERROR) !BCAST IT 

      !Determine my processes rank WITHIN THIS COMMUNICATION GROUP
      CALL MPI_COMM_RANK(WORK_GROUP%COMP_ENVIRONMENT%MPI_COMM,RANK,MPI_IERROR) 
      CALL MPI_ERROR_CHECK("MPI_COMM_RANK",MPI_IERROR,ERR,ERROR,*999)
      WORK_GROUP%COMP_ENVIRONMENT%MY_COMPUTATIONAL_NODE_NUMBER=RANK

  !---------------------------------------------------------------    
      !fill the information for every computational node in this group
      ALLOCATE(WORK_GROUP%COMP_ENVIRONMENT%COMPUTATIONAL_NODES(0:WORK_GROUP%COMP_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES-1))

      CALL COMPUTATIONAL_NODE_MPI_TYPE_INITIALISE(WORK_GROUP%COMP_ENVIRONMENT%COMPUTATIONAL_NODES(RANK),ERR,ERROR,*999)
      CALL COMPUTATIONAL_NODE_INITIALISE(WORK_GROUP%COMP_ENVIRONMENT%COMPUTATIONAL_NODES(0),RANK,ERR,ERROR,*999)
      !Now transfer all the computational node information to the other computational nodes 
      CALL MPI_ALLGATHER(WORK_GROUP%COMP_ENVIRONMENT%COMPUTATIONAL_NODES(0),1,MPI_COMPUTATIONAL_NODE_TYPE_DATA%MPI_TYPE, &
        & WORK_GROUP%COMP_ENVIRONMENT%COMPUTATIONAL_NODES(0),1,MPI_COMPUTATIONAL_NODE_TYPE_DATA%MPI_TYPE, &
        & WORK_GROUP%COMP_ENVIRONMENT%MPI_COMM,MPI_IERROR)
      CALL MPI_ERROR_CHECK("MPI_ALLGATHER",MPI_IERROR,ERR,ERROR,*999)
      !bookkeep this local work group, one rank only belongs to one local group currnetly
      IF(ASSOCIATED(COMPUTATIONAL_LOCAL_WORK_GROUP)) THEN
        CALL FLAG_ERROR('COMPUTATIONAL_LOCAL_WORK_GROUP is already associated,  & 
          & possible reason: bug or one rank only belongs to one local group currnetly', ERR, ERROR, *999)
      ELSE
        COMPUTATIONAL_LOCAL_WORK_GROUP => WORK_GROUP 
      ENDIF
  
    ENDIF
    CALL MPI_BARRIER(MPI_COMM_WORLD, MPI_IERROR)

    IF(ORIGINAL_GROUP/=MPI_GROUP_NULL)     CALL MPI_GROUP_FREE(ORIGINAL_GROUP,MPI_IERROR)
    CALL MPI_ERROR_CHECK("MPI_GROUP_FREE",MPI_IERROR,ERR,ERROR,*999)    
    IF(NEW_GROUP/=MPI_GROUP_NULL)     CALL MPI_GROUP_FREE(NEW_GROUP,MPI_IERROR) 
    CALL MPI_ERROR_CHECK("MPI_COMM_RANK",MPI_IERROR,ERR,ERROR,*999)    

    !SHRINK THE AVAILABLE_RANK_LIST
    ALLOCATE(NEW_AVAILABLE_RANK_LIST(SIZE(AVAILABLE_RANK_LIST,1)-WORK_GROUP%COMP_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES))  
    NEW_AVAILABLE_RANK_LIST(1:SIZE(NEW_AVAILABLE_RANK_LIST)) = AVAILABLE_RANK_LIST(1+WORK_GROUP%COMP_ENVIRONMENT &
      &  %NUMBER_COMPUTATIONAL_NODES:SIZE(NEW_AVAILABLE_RANK_LIST,1)+WORK_GROUP%COMP_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES)
    DEALLOCATE(AVAILABLE_RANK_LIST)
    ALLOCATE(AVAILABLE_RANK_LIST(SIZE(NEW_AVAILABLE_RANK_LIST,1)))
    AVAILABLE_RANK_LIST(:) = NEW_AVAILABLE_RANK_LIST(:)

    WORK_GROUP%COMP_ENV_FINISHED = .TRUE.

    !Recursively do so to all its subgroups
    DO I=1,WORK_GROUP%NUMBER_SUB_WORK_GROUPS,1
      CALL COMPUTATIONAL_WORK_GROUP_GENERATE_COMP_ENVIRONMENT(WORK_GROUP%SUB_WORK_GROUPS(I)%PTR,&
        & AVAILABLE_RANK_LIST,ERR,ERROR,*999)      
    END DO

    CALL EXITS("COMPUTATIONAL_WORK_GROUP_GENERATE_COMP_ENVIRONMENT")
    RETURN
999 CALL ERRORS("COMPUTATIONAL_WORK_GROUP_GENERATE_COMP_ENVIRONMENT",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_WORK_GROUP_GENERATE_COMP_ENVIRONMENT")
    RETURN 1
  END SUBROUTINE COMPUTATIONAL_WORK_GROUP_GENERATE_COMP_ENVIRONMENT

  !
  !================================================================================================================================
  !
  !>
  !>GENERATE THE HIERARCHY COMPUTATIONAL ENVIRONMENT BASED ON WORK GROUP TREE
  !>
  SUBROUTINE COMPUTATIONAL_WORK_GROUP_CREATE_FINISH(WORLD_WORK_GROUP,ERR,ERROR,*)
    !Argument Variables
    TYPE(COMPUTATIONAL_WORK_GROUP_TYPE),POINTER,INTENT(INOUT) :: WORLD_WORK_GROUP
    INTEGER(INTG),INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING),INTENT(OUT) :: ERROR !<The error string
    !LOCAL VARIABLES 
    INTEGER(INTG),ALLOCATABLE:: AVAILABLE_RANK_LIST(:)
    INTEGER(INTG) :: I, BCAST_VALUE,MPI_IERROR

    CALL ENTERS("COMPUTATIONAL_WORK_GROUP_CREATE_FINISH",ERR,ERROR,*999)

    !set the computational environment of the world work group to be the global COMPUTATIONAL_ENVIRONMENT
    !(the default communicator in opencmiss)
    WORLD_WORK_GROUP%COMP_ENVIRONMENT => COMPUTATIONAL_ENVIRONMENT 
    WORLD_WORK_GROUP%COMP_ENV_FINISHED = .TRUE.

    NULLIFY(COMPUTATIONAL_LOCAL_WORK_GROUP) 

    !generate the communicators for subgroups if any
    ALLOCATE(AVAILABLE_RANK_LIST(WORLD_WORK_GROUP%COMP_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES))
    DO I=0,SIZE(AVAILABLE_RANK_LIST,1)-1
      AVAILABLE_RANK_LIST(I+1) = I
    END DO

    IF(WORLD_WORK_GROUP%NUMBER_SUB_WORK_GROUPS>0) THEN
      IF(DIAGNOSTICS5) THEN
        WRITE(OP_STRING,'("creating (I) work groups from (I) ranks")')  WORLD_WORK_GROUP%NUMBER_SUB_WORK_GROUPS, &
         & SIZE(AVAILABLE_RANK_LIST,1)
        CALL WRITE_STR(DIAGNOSTIC_OUTPUT_TYPE,ERR,ERROR,*999)
      ENDIF          
      DO I=1,WORLD_WORK_GROUP%NUMBER_SUB_WORK_GROUPS,1
        CALL COMPUTATIONAL_WORK_GROUP_GENERATE_COMP_ENVIRONMENT(&
          & WORLD_WORK_GROUP%SUB_WORK_GROUPS(I)%PTR,AVAILABLE_RANK_LIST,ERR,ERROR,*999) 
      END DO
    ELSE !DEFAULT: USE WORKLD WORK GROUP
      COMPUTATIONAL_LOCAL_WORK_GROUP=>WORLD_WORK_GROUP
      WORLD_WORK_GROUP%NUMBER_COMPUTATIONAL_NODES=WORLD_WORK_GROUP%COMP_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES
    ENDIF

    !double check that each rank belongs to a local group
    IF (WORLD_WORK_GROUP%NUMBER_SUB_WORK_GROUPS>0) THEN
      IF(ASSOCIATED(COMPUTATIONAL_LOCAL_WORK_GROUP)) THEN
        IF(DIAGNOSTICS5) THEN
          WRITE(OP_STRING,'("rank (I) belong to one group")')  COMPUTATIONAL_ENVIRONMENT%MY_COMPUTATIONAL_NODE_NUMBER
          CALL WRITE_STR(DIAGNOSTIC_OUTPUT_TYPE,ERR,ERROR,*999)
        ENDIF          
      !temporary solution,set COMPUTATIONAL_ENVIRONMENT to be local!
      !COMPUTATIONAL_ENVIRONMENT = COMPUTATIONAL_LOCAL_WORK_GROUP%COMP_ENVIRONMENT
      CALL PETSC_FINALIZE(ERR,ERROR,*999) !(re)initialise PETSc
      PETSC_COMM_WORLD=COMPUTATIONAL_LOCAL_WORK_GROUP%COMP_ENVIRONMENT%MPI_COMM
      CALL PETSC_INITIALIZE(PETSC_NULL_CHARACTER,ERR,ERROR,*999) 
      ELSE
        IF(DIAGNOSTICS5) THEN
          WRITE(OP_STRING,'("rank (I) does not belong to any local group, is that a bug?")')  &
           & COMPUTATIONAL_ENVIRONMENT%MY_COMPUTATIONAL_NODE_NUMBER
          CALL WRITE_STR(DIAGNOSTIC_OUTPUT_TYPE,ERR,ERROR,*999)
        ENDIF          
        CALL FLAG_ERROR('COMPUTATIONAL_LOCAL_WORK_GROUP IS NOT ASSOCIATED',ERR,ERROR,*999)
      ENDIF
    ENDIF

    COMPUTATIONAL_WORLD_WORK_GROUP => WORLD_WORK_GROUP

    CALL EXITS("COMPUTATIONAL_WORK_GROUP_CREATE_FINISH")
    RETURN
999 CALL ERRORS("COMPUTATIONAL_WORK_GROUP_CREATE_FINISH",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_WORK_GROUP_CREATE_FINISH")
    RETURN 1
  END SUBROUTINE COMPUTATIONAL_WORK_GROUP_CREATE_FINISH

  !
  !================================================================================================================================
  !

  !>Finalises the computational node data structures and deallocates all memory.
  SUBROUTINE COMPUTATIONAL_NODE_FINALISE(COMPUTATIONAL_NODE,ERR,ERROR,*)
  
    !Argument Variables
    TYPE(COMPUTATIONAL_NODE_TYPE),INTENT(INOUT) :: COMPUTATIONAL_NODE !<The computational node to finalise
    INTEGER(INTG),INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING),INTENT(OUT) :: ERROR !<The error string
    !Local Variables

    CALL ENTERS("COMPUTATIONAL_NODE_FINALISE",ERR,ERROR,*999)

    COMPUTATIONAL_NODE%NUMBER_PROCESSORS=0
    COMPUTATIONAL_NODE%RANK=-1
    COMPUTATIONAL_NODE%NODE_NAME_LENGTH=0
    COMPUTATIONAL_NODE%NODE_NAME=""    

    CALL EXITS("COMPUTATIONAL_NODE_FINALISE")
    RETURN
999 CALL ERRORS("COMPUTATIONAL_NODE_FINALISE",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_NODE_FINALISE")
    RETURN 1
  END SUBROUTINE COMPUTATIONAL_NODE_FINALISE

  !
  !================================================================================================================================
  !

  !>Initialises the computational node data structures.
  SUBROUTINE COMPUTATIONAL_NODE_INITIALISE(COMPUTATIONAL_NODE,RANK,ERR,ERROR,*)
  
    !Argument Variables
    TYPE(COMPUTATIONAL_NODE_TYPE), INTENT(OUT) :: COMPUTATIONAL_NODE !<The computational node to initialise
    INTEGER(INTG), INTENT(IN) :: RANK !<The MPI rank of the computational node
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: MPI_IERROR

    CALL ENTERS("COMPUTATIONAL_NODE_INITIALISE",ERR,ERROR,*999)

!    COMPUTATIONAL_NODE%NUMBER_PROCESSORS=COMP_DETECT_NUMBER_PROCESSORS(ERR)
!    IF(ERR/=0) GOTO 999
    COMPUTATIONAL_NODE%NUMBER_PROCESSORS=1
    COMPUTATIONAL_NODE%RANK=RANK
    CALL MPI_GET_PROCESSOR_NAME(COMPUTATIONAL_NODE%NODE_NAME,COMPUTATIONAL_NODE%NODE_NAME_LENGTH,MPI_IERROR)
    CALL MPI_ERROR_CHECK("MPI_GET_PROCESSOR_NAME",MPI_IERROR,ERR,ERROR,*999)
    
    CALL EXITS("COMPUTATIONAL_NODE_INITIALISE")
    RETURN
999 CALL ERRORS("COMPUTATIONAL_NODE_INITIALISE",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_NODE_INITIALISE")
    RETURN 1
  END SUBROUTINE COMPUTATIONAL_NODE_INITIALISE

  !
  !================================================================================================================================
  !

  !>Finalises the data structure containing the MPI type information for the COMPUTATIONAL_NODE_TYPE.
  SUBROUTINE COMPUTATIONAL_NODE_MPI_TYPE_FINALISE(ERR,ERROR,*)
  
    !Argument Variables
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i,MPI_IERROR

    CALL ENTERS("COMPUTATIONAL_NODE_MPI_TYPE_FINALISE",ERR,ERROR,*999)

    DO i=1,MPI_COMPUTATIONAL_NODE_TYPE_DATA%NUM_BLOCKS
      MPI_COMPUTATIONAL_NODE_TYPE_DATA%TYPES(i)=0
      MPI_COMPUTATIONAL_NODE_TYPE_DATA%BLOCK_LENGTHS(i)=0
      MPI_COMPUTATIONAL_NODE_TYPE_DATA%DISPLACEMENTS(i)=0
    ENDDO !i
    MPI_COMPUTATIONAL_NODE_TYPE_DATA%NUM_BLOCKS=0

    IF(MPI_COMPUTATIONAL_NODE_TYPE_DATA%MPI_TYPE/=MPI_DATATYPE_NULL) THEN
      CALL MPI_TYPE_FREE(MPI_COMPUTATIONAL_NODE_TYPE_DATA%MPI_TYPE,MPI_IERROR)
      CALL MPI_ERROR_CHECK("MPI_TYPE_FREE",MPI_IERROR,ERR,ERROR,*999)
    ENDIF

    CALL EXITS("COMPUTATIONAL_NODE_MPI_TYPE_FINALISE")
    RETURN
999 CALL ERRORS("COMPUTATIONAL_NODE_MPI_TYPE_FINALISE",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_NODE_MPI_TYPE_FINALISE")
    RETURN 1
  END SUBROUTINE COMPUTATIONAL_NODE_MPI_TYPE_FINALISE

  !
  !================================================================================================================================
  !

  !>Initialises the data structure containing the MPI type information for the COMPUTATIONAL_NODE_TYPE.
  SUBROUTINE COMPUTATIONAL_NODE_MPI_TYPE_INITIALISE(COMPUTATIONAL_NODE,ERR,ERROR,*)
  
    !Argument Variables
    TYPE(COMPUTATIONAL_NODE_TYPE), INTENT(IN) :: COMPUTATIONAL_NODE !<The computational node containing the MPI type to initialise
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: I,MPI_IERROR

    CALL ENTERS("COMPUTATIONAL_NODE_MPI_TYPE_INITIALISE",ERR,ERROR,*999)

    MPI_COMPUTATIONAL_NODE_TYPE_DATA%MPI_TYPE=MPI_DATATYPE_NULL
    
    MPI_COMPUTATIONAL_NODE_TYPE_DATA%NUM_BLOCKS=4
    MPI_COMPUTATIONAL_NODE_TYPE_DATA%TYPES=(/MPI_INTEGER,MPI_INTEGER,MPI_INTEGER,MPI_CHARACTER/)
    MPI_COMPUTATIONAL_NODE_TYPE_DATA%BLOCK_LENGTHS=(/1,1,1,MPI_MAX_PROCESSOR_NAME/)
        
        
    CALL MPI_GET_ADDRESS(COMPUTATIONAL_NODE%NUMBER_PROCESSORS,MPI_COMPUTATIONAL_NODE_TYPE_DATA%DISPLACEMENTS(1),MPI_IERROR)
    CALL MPI_ERROR_CHECK("MPI_GET_ADDRESS",MPI_IERROR,ERR,ERROR,*999)
    CALL MPI_GET_ADDRESS(COMPUTATIONAL_NODE%RANK,MPI_COMPUTATIONAL_NODE_TYPE_DATA%DISPLACEMENTS(2),MPI_IERROR)
    CALL MPI_ERROR_CHECK("MPI_GET_ADDRESS",MPI_IERROR,ERR,ERROR,*999)
    CALL MPI_GET_ADDRESS(COMPUTATIONAL_NODE%NODE_NAME_LENGTH,MPI_COMPUTATIONAL_NODE_TYPE_DATA%DISPLACEMENTS(3),MPI_IERROR)
    CALL MPI_ERROR_CHECK("MPI_GET_ADDRESS",MPI_IERROR,ERR,ERROR,*999)
    !CPB 19/02/07 AIX compiler complains about the type of the first parameter i.e., the previous 3 have been integers
    !and this one is not so cast the type.
    CALL MPI_GET_ADDRESS(COMPUTATIONAL_NODE%NODE_NAME,MPI_COMPUTATIONAL_NODE_TYPE_DATA%DISPLACEMENTS(4),MPI_IERROR)
    CALL MPI_ERROR_CHECK("MPI_GET_ADDRESS",MPI_IERROR,ERR,ERROR,*999)

    DO i=4,1,-1
      MPI_COMPUTATIONAL_NODE_TYPE_DATA%DISPLACEMENTS(I)=MPI_COMPUTATIONAL_NODE_TYPE_DATA%DISPLACEMENTS(I)- &
        & MPI_COMPUTATIONAL_NODE_TYPE_DATA%DISPLACEMENTS(1)
    ENDDO !i

    CALL MPI_TYPE_CREATE_STRUCT(MPI_COMPUTATIONAL_NODE_TYPE_DATA%NUM_BLOCKS,MPI_COMPUTATIONAL_NODE_TYPE_DATA%BLOCK_LENGTHS, &
      & MPI_COMPUTATIONAL_NODE_TYPE_DATA%DISPLACEMENTS,MPI_COMPUTATIONAL_NODE_TYPE_DATA%TYPES, &
      & MPI_COMPUTATIONAL_NODE_TYPE_DATA%MPI_TYPE,MPI_IERROR)
    CALL MPI_ERROR_CHECK("MPI_TYPE_CREATE_STRUCT",MPI_IERROR,ERR,ERROR,*999)

    CALL MPI_TYPE_COMMIT(MPI_COMPUTATIONAL_NODE_TYPE_DATA%MPI_TYPE, MPI_IERROR)
    CALL MPI_ERROR_CHECK("MPI_TYPE_COMMIT",MPI_IERROR,ERR,ERROR,*999)
    
    IF(DIAGNOSTICS3) THEN
      CALL WRITE_STRING(DIAGNOSTIC_OUTPUT_TYPE,"MPI Computational Node Type Data:",ERR,ERROR,*999)
      CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  MPI type = ",MPI_COMPUTATIONAL_NODE_TYPE_DATA%MPI_TYPE,ERR,ERROR,*999)
      CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Number blocks  = ",MPI_COMPUTATIONAL_NODE_TYPE_DATA%NUM_BLOCKS, &
        & ERR,ERROR,*999)
      CALL WRITE_STRING_VECTOR(DIAGNOSTIC_OUTPUT_TYPE,1,1,MPI_COMPUTATIONAL_NODE_TYPE_DATA%NUM_BLOCKS,4,4, &
        & MPI_COMPUTATIONAL_NODE_TYPE_DATA%TYPES,'("  Block types =",4(X,I15))','(15X,4(X,I15))',ERR,ERROR,*999)
      CALL WRITE_STRING_VECTOR(DIAGNOSTIC_OUTPUT_TYPE,1,1,MPI_COMPUTATIONAL_NODE_TYPE_DATA%NUM_BLOCKS,8,8, &
        & MPI_COMPUTATIONAL_NODE_TYPE_DATA%BLOCK_LENGTHS,'("  Block lengths =",8(X,I5))','(17X,8(X,I5))',ERR,ERROR,*999)
      CALL WRITE_STRING_VECTOR(DIAGNOSTIC_OUTPUT_TYPE,1,1,MPI_COMPUTATIONAL_NODE_TYPE_DATA%NUM_BLOCKS,8,8, &
        & MPI_COMPUTATIONAL_NODE_TYPE_DATA%DISPLACEMENTS,'("  Displacements =",8(X,I5))','(17X,8(X,I5))',ERR,ERROR,*999)
    ENDIF

    CALL EXITS("COMPUTATIONAL_NODE_MPI_TYPE_INITIALISE")
    RETURN
999 CALL COMPUTATIONAL_NODE_MPI_TYPE_FINALISE(ERR,ERROR,*998)
998 CALL ERRORS("COMPUTATIONAL_NODE_MPI_TYPE_INITIALISE",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_NODE_MPI_TYPE_INITIALISE")
    RETURN 1
  END SUBROUTINE COMPUTATIONAL_NODE_MPI_TYPE_INITIALISE

  !
  !================================================================================================================================
  !
  
  !>
  !>Finalises the computational environment data structures and deallocates all memory.
  !>
  SUBROUTINE COMPUTATIONAL_ENVIRONMENT_FINALISE(ERR,ERROR,*)
  
    !Argument Variables
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: COMPUTATIONAL_NODE,MPI_IERROR

    CALL ENTERS("COMPUTATIONAL_ENVIRONMENT_FINALISE",ERR,ERROR,*999)

    !Finalise PetSc
    !CALL PETSC_LOGPRINTSUMMARY(PETSC_COMM_WORLD,"OpenCMISSTest.petsc",ERR,ERROR,*999)
    CALL PETSC_FINALIZE(ERR,ERROR,*999)
    
    IF(ALLOCATED(COMPUTATIONAL_ENVIRONMENT%COMPUTATIONAL_NODES)) THEN
       DO COMPUTATIONAL_NODE=0,COMPUTATIONAL_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES-1
          CALL COMPUTATIONAL_NODE_FINALISE(COMPUTATIONAL_ENVIRONMENT%COMPUTATIONAL_NODES(COMPUTATIONAL_NODE),ERR,ERROR,*999)
       ENDDO
       DEALLOCATE(COMPUTATIONAL_ENVIRONMENT%COMPUTATIONAL_NODES)
    ENDIF
    COMPUTATIONAL_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES=0

    !TODO: DEALLOCATE WORK GROUP TREE AND THE MPI COMMUNICATORS ASSOCIATED

    CALL COMPUTATIONAL_NODE_MPI_TYPE_FINALISE(ERR,ERROR,*999)

    CALL MPI_COMM_FREE(COMPUTATIONAL_ENVIRONMENT%MPI_COMM,MPI_IERROR)
    CALL MPI_ERROR_CHECK("MPI_COMM_FREE",MPI_IERROR,ERR,ERROR,*999)
    
    CALL MPI_FINALIZE(MPI_IERROR)
    CALL MPI_ERROR_CHECK("MPI_FINALIZE",MPI_IERROR,ERR,ERROR,*999)

    CALL EXITS("COMPUTATIONAL_ENVIRONMENT_FINALISE")
    RETURN
999 CALL ERRORS("COMPUTATIONAL_ENVIRONMENT_FINALISE",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_ENVIRONMENT_FINALISE")
    RETURN 1
  END SUBROUTINE COMPUTATIONAL_ENVIRONMENT_FINALISE

  !
  !================================================================================================================================
  !

  !>
  !>Initialises the computational environment data structures.
  !>
  SUBROUTINE COMPUTATIONAL_ENVIRONMENT_INITIALISE(ERR,ERROR,*)
  
    !Argument Variables
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i,DUMMY_ERR,MPI_IERROR,RANK
    TYPE(VARYING_STRING) :: DUMMY_ERROR

    CALL ENTERS("COMPUTATIONAL_ENVIRONMENT_INITIALISE",ERR,ERROR,*999)

    !Initialise the MPI environment
    CALL MPI_INIT(MPI_IERROR)
    CALL MPI_ERROR_CHECK("MPI_INIT",MPI_IERROR,ERR,ERROR,*999)

    !Create a (private) communicator for cmiss. For now just duplicate MPI_COMM_WORLD
    CALL MPI_COMM_DUP(MPI_COMM_WORLD,COMPUTATIONAL_ENVIRONMENT%MPI_COMM,MPI_IERROR)
    CALL MPI_ERROR_CHECK("MPI_COMM_DUP",MPI_IERROR,ERR,ERROR,*999)
    
    !Determine the number of ranks/computational nodes we have in our computational environment
    CALL MPI_COMM_SIZE(COMPUTATIONAL_ENVIRONMENT%MPI_COMM,COMPUTATIONAL_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES,MPI_IERROR)
    CALL MPI_ERROR_CHECK("MPI_COMM_SIZE",MPI_IERROR,ERR,ERROR,*999)

    !Allocate the computational node data structures
    ALLOCATE(COMPUTATIONAL_ENVIRONMENT%COMPUTATIONAL_NODES(0:COMPUTATIONAL_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES-1),STAT=ERR)
    IF(ERR /=0) CALL FLAG_ERROR("Could not allocate computational nodes",ERR,ERROR,*999)

    !Determine my processes rank
    CALL MPI_COMM_RANK(COMPUTATIONAL_ENVIRONMENT%MPI_COMM,RANK,MPI_IERROR)
    CALL MPI_ERROR_CHECK("MPI_COMM_RANK",MPI_IERROR,ERR,ERROR,*999)
    COMPUTATIONAL_ENVIRONMENT%MY_COMPUTATIONAL_NODE_NUMBER=RANK
    
#ifdef TAUPROF
    CALL TAU_PROFILE_SET_NODE(rank)
#endif

    !Create the MPI type information for the COMPUTATIONAL_NODE_TYPE
    CALL COMPUTATIONAL_NODE_MPI_TYPE_INITIALISE(COMPUTATIONAL_ENVIRONMENT%COMPUTATIONAL_NODES(RANK),ERR,ERROR,*999)
    !Fill in all the computational node data structures for this rank at the root position (will be changed later with an
    !allgather call)
    CALL COMPUTATIONAL_NODE_INITIALISE(COMPUTATIONAL_ENVIRONMENT%COMPUTATIONAL_NODES(0),RANK,ERR,ERROR,*999)

    !Now transfer all the computational node information to the other computational nodes so that each rank has all the
    !information.
    CALL MPI_ALLGATHER(COMPUTATIONAL_ENVIRONMENT%COMPUTATIONAL_NODES(0),1,MPI_COMPUTATIONAL_NODE_TYPE_DATA%MPI_TYPE, &
      & COMPUTATIONAL_ENVIRONMENT%COMPUTATIONAL_NODES(0),1,MPI_COMPUTATIONAL_NODE_TYPE_DATA%MPI_TYPE, &
      & COMPUTATIONAL_ENVIRONMENT%MPI_COMM,MPI_IERROR)
    CALL MPI_ERROR_CHECK("MPI_ALLGATHER",MPI_IERROR,ERR,ERROR,*999)

    !Initialise node numbers in base routines. (It needs to be reset later if the the work group is not world work group.)
    CALL COMPUTATIONAL_NODE_NUMBERS_SET(COMPUTATIONAL_ENVIRONMENT%MY_COMPUTATIONAL_NODE_NUMBER,COMPUTATIONAL_ENVIRONMENT% &
      & NUMBER_COMPUTATIONAL_NODES,ERR,ERROR,*999)
    
    !Initialise PETSc
    CALL PETSC_INITIALIZE(PETSC_NULL_CHARACTER,ERR,ERROR,*999)
    
    IF(DIAGNOSTICS1) THEN
      !Just let the master node write out this information
      IF(RANK==0) THEN
        CALL WRITE_STRING(DIAGNOSTIC_OUTPUT_TYPE,"Computational environment:",ERR,ERROR,*999)
        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Number of computational nodes = ", &
          & COMPUTATIONAL_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES,ERR,ERROR,*999)
        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  My computational node number = ", &
          & COMPUTATIONAL_ENVIRONMENT%MY_COMPUTATIONAL_NODE_NUMBER,ERR,ERROR,*999)
        IF(DIAGNOSTICS2) THEN
          DO i=0,COMPUTATIONAL_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES-1
            CALL WRITE_STRING(DIAGNOSTIC_OUTPUT_TYPE,"  Computational Node:",ERR,ERROR,*999)
            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"    Number of Processors = ", &
              & COMPUTATIONAL_ENVIRONMENT%COMPUTATIONAL_NODES(i)%NUMBER_PROCESSORS,ERR,ERROR,*999)
            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"    MPI rank = ", &
             & COMPUTATIONAL_ENVIRONMENT%COMPUTATIONAL_NODES(i)%RANK,ERR,ERROR,*999)
            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"    Node Name = ", &
              & COMPUTATIONAL_ENVIRONMENT%COMPUTATIONAL_NODES(i)%NODE_NAME,ERR,ERROR,*999)
          ENDDO !i
        ENDIF
      ENDIF
    ENDIF

    !default
    NULLIFY(COMPUTATIONAL_LOCAL_WORK_GROUP)
    NULLIFY(COMPUTATIONAL_WORLD_WORK_GROUP)
    CALL COMPUTATIONAL_WORK_GROUP_CREATE_START(0, COMPUTATIONAL_WORLD_WORK_GROUP,0,ERR,ERROR,*999)
    CALL COMPUTATIONAL_WORK_GROUP_CREATE_FINISH(COMPUTATIONAL_WORLD_WORK_GROUP, ERR,ERROR,*999)


    CALL EXITS("COMPUTATIONAL_ENVIRONMENT_INITIALISE")
    RETURN
999 CALL COMPUTATIONAL_ENVIRONMENT_FINALISE(DUMMY_ERR,DUMMY_ERROR,*998)
998 CALL ERRORS("COMPUTATIONAL_ENVIRONMENT_INITIALISE",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_ENVIRONMENT_INITIALISE")
    RETURN 1
  END SUBROUTINE COMPUTATIONAL_ENVIRONMENT_INITIALISE

  !
  !================================================================================================================================
  !

  !>Set a barrier in world group
  SUBROUTINE COMPUTATIONAL_WORK_GROUP_WAIT_WORLD_WORKGROUP(ERR,ERROR)
      
    !Argument Variables
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local variables
    INTEGER(INTG) :: MPI_IERROR

    CALL ENTERS("COMPUTATIONAL_WORK_GROUP_WAIT_WORLD_WORKGROUP",ERR,ERROR,*999)

    CALL MPI_BARRIER(COMPUTATIONAL_ENVIRONMENT%MPI_COMM, MPI_IERROR)
    CALL MPI_ERROR_CHECK("MPI_ALLGATHER",MPI_IERROR,ERR,ERROR,*999)

    CALL EXITS("COMPUTATIONAL_WORK_GROUP_WAIT_WORLD_WORKGROUP")
    RETURN
999 CALL ERRORS("COMPUTATIONAL_WORK_GROUP_WAIT_WORLD_WORKGROUP",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_WORK_GROUP_WAIT_WORLD_WORKGROUP")
    RETURN 
  END SUBROUTINE COMPUTATIONAL_WORK_GROUP_WAIT_WORLD_WORKGROUP


  !
  !================================================================================================================================
  !

  !>Returns the rank in world group.
  FUNCTION COMPUTATIONAL_NODE_NUMBER_GET_WORLD_WORKGROUP(ERR,ERROR)
      
    !Argument Variables
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Function variable
    INTEGER(INTG) :: COMPUTATIONAL_NODE_NUMBER_GET_WORLD_WORKGROUP !<On exit the computational node rank of the current process.
    !Local Variables

    CALL ENTERS("COMPUTATIONAL_NODE_NUMBER_GET_WORLD_WORKGROUP",ERR,ERROR,*999)

    IF(ALLOCATED(COMPUTATIONAL_ENVIRONMENT%COMPUTATIONAL_NODES)) THEN
      COMPUTATIONAL_NODE_NUMBER_GET_WORLD_WORKGROUP=COMPUTATIONAL_ENVIRONMENT%MY_COMPUTATIONAL_NODE_NUMBER
    ELSE
      CALL FLAG_ERROR("Computational environment not initialised",ERR,ERROR,*999)
    ENDIF
    
    CALL EXITS("COMPUTATIONAL_NODE_NUMBER_GET_WORLD_WORKGROUP")
    RETURN
999 CALL ERRORS("COMPUTATIONAL_NODE_NUMBER_GET_WORLD_WORKGROUP",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_NODE_NUMBER_GET_WORLD_WORKGROUP")
    RETURN 
  END FUNCTION COMPUTATIONAL_NODE_NUMBER_GET_WORLD_WORKGROUP

  !
  !================================================================================================================================
  !
  
  !>Returns the size of world work group.
  FUNCTION COMPUTATIONAL_NODES_NUMBER_GET_WORLD_WORKGROUP(ERR,ERROR)
     
    !Argument Variables
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Function variable
    INTEGER(INTG) :: COMPUTATIONAL_NODES_NUMBER_GET_WORLD_WORKGROUP !<On exit, the number of computational nodes for the program.
    !Local Variables

    CALL ENTERS("COMPUTATIONAL_NODES_NUMBER_GET_WORLD_WORKGROUP",ERR,ERROR,*999)

    IF(ALLOCATED(COMPUTATIONAL_ENVIRONMENT%COMPUTATIONAL_NODES)) THEN
      COMPUTATIONAL_NODES_NUMBER_GET_WORLD_WORKGROUP=COMPUTATIONAL_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES
    ELSE
      CALL FLAG_ERROR("Computational environment not initialised",ERR,ERROR,*999)
    ENDIF
    
    CALL EXITS("COMPUTATIONAL_NODES_NUMBER_GET_WORLD_WORKGROUP")
    RETURN
999 CALL ERRORS("COMPUTATIONAL_NODES_NUMBER_GET_WORLD_WORKGROUP",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_NODES_NUMBER_GET_WORLD_WORKGROUP")
    RETURN 
  END FUNCTION COMPUTATIONAL_NODES_NUMBER_GET_WORLD_WORKGROUP

  !
  !================================================================================================================================
  !

  !>Returns the rank in local work group.
  FUNCTION COMPUTATIONAL_NODE_NUMBER_GET_WORKGROUP(WORK_GROUP, ERR,ERROR)
      
    !Argument Variables
    TYPE(COMPUTATIONAL_WORK_GROUP_TYPE),POINTER, INTENT(IN) :: WORK_GROUP
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Function variable
    INTEGER(INTG) :: COMPUTATIONAL_NODE_NUMBER_GET_WORKGROUP !<On exit the computational node number/rank of the current process.
    !Local Variables

    CALL ENTERS("COMPUTATIONAL_NODE_NUMBER_GET_WORKGROUP",ERR,ERROR,*999)

    IF(ASSOCIATED(WORK_GROUP)) THEN 
      IF(WORK_GROUP%COMP_ENV_FINISHED) THEN
        COMPUTATIONAL_NODE_NUMBER_GET_WORKGROUP = WORK_GROUP%COMP_ENVIRONMENT%MY_COMPUTATIONAL_NODE_NUMBER
      ELSE
        CALL FLAG_ERROR('computational environment of this work group has not finished', ERR, ERROR, *999)
      ENDIF
    ELSE
      CALL FLAG_ERROR('WORK_GROUP is not associated', ERR, ERROR, *999)
    ENDIF

    CALL EXITS("COMPUTATIONAL_NODE_NUMBER_GET_WORKGROUP")
    RETURN
999 CALL ERRORS("COMPUTATIONAL_NODE_NUMBER_GET_WORKGROUP",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_NODE_NUMBER_GET_WORKGROUP")
    RETURN 
  END FUNCTION COMPUTATIONAL_NODE_NUMBER_GET_WORKGROUP

  !
  !================================================================================================================================
  !
  
  !>Returns the size of local work group.
  FUNCTION COMPUTATIONAL_NODES_NUMBER_GET_WORKGROUP(WORK_GROUP,ERR,ERROR)
     
    !Argument Variables
    TYPE(COMPUTATIONAL_WORK_GROUP_TYPE),POINTER, INTENT(IN) :: WORK_GROUP
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Function variable
    INTEGER(INTG) :: COMPUTATIONAL_NODES_NUMBER_GET_WORKGROUP !<On exit, the number of computational nodes for the program.
    !Local Variables

    CALL ENTERS("COMPUTATIONAL_NODES_NUMBER_GET_WORKGROUP",ERR,ERROR,*999)

    IF(ASSOCIATED(WORK_GROUP)) THEN 
      IF(WORK_GROUP%COMP_ENV_FINISHED) THEN
        COMPUTATIONAL_NODES_NUMBER_GET_WORKGROUP = WORK_GROUP%COMP_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES
        !double check
        IF(WORK_GROUP%COMP_ENVIRONMENT%NUMBER_COMPUTATIONAL_NODES /= WORK_GROUP%NUMBER_COMPUTATIONAL_NODES) THEN
          CALL FLAG_ERROR('inconsistency between number of computational nodes between WORK_GROUP and WORK_GROUP%COMP_ENV',  &
          & ERR, ERROR, *999)
        ENDIF
      ELSE
        CALL FLAG_ERROR('computational environment of this work group has not finished', ERR, ERROR, *999)
      ENDIF
    ELSE
      CALL FLAG_ERROR('WORK_GROUP is not associated', ERR, ERROR, *999)
    ENDIF
    
    CALL EXITS("COMPUTATIONAL_NODES_NUMBER_GET_WORKGROUP")
    RETURN
999 CALL ERRORS("COMPUTATIONAL_NODES_NUMBER_GET_WORKGROUP",ERR,ERROR)
    CALL EXITS("COMPUTATIONAL_NODES_NUMBER_GET_WORKGROUP")
    RETURN 
  END FUNCTION COMPUTATIONAL_NODES_NUMBER_GET_WORKGROUP

  !
  !================================================================================================================================
  !

END MODULE COMP_ENVIRONMENT
